## Rajendra (RB) - 8.06.2025
""" 

This Program is a classic application of LAngchain and Pydantic 
To call the various LLM and RAG and uses the query and edges

Nodes “retriever” and “generate” are incorporated into the workflow to represent distinct functions that cyclically
interact during processing. An edge is established from “retriever” to “generate” to signify the flow of data between these nodes.


The entry point is designated as “retriever,” marking the starting point of the workflow, while “generate” serves as the finish point where the final response is generated.

The workflow undergoes compilation into an executable application ready for processing input data. 
An initial message regarding news on LangGraph is provided to initiate the workflow’s execution.

During execution, outputs from each node are systematically printed to track the processing steps.
 The final response generated by the workflow encapsulates the essence of all preceding interactions, culminating in a comprehensive output for further analysis or action.

In the next note, we will explore expanding basic RAG to more advanced RAG pipeline.


## """


print("all ok")

print("-"*100)

import operator
import os
from typing import List
from pydantic import BaseModel , Field
from langchain_core.prompts import PromptTemplate
from typing import TypedDict, Annotated, Sequence
from langchain_core.messages import BaseMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.graph import StateGraph,END

## This workbook is fully executed and it works
import langchain
langchain.verbose = False
langchain.debug = False
langchain.llm_cache = False

from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o", temperature=0.2)
output=model.invoke("hi")
print(output.content)
print("-"*100)
print("\n")




from langchain_huggingface import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en")
len(embeddings.embed_query("hi"))
print("\n")
print("-"*100)


print("Embedding model is completed...")
print("\n")
print("-"*100)

#Configure the embedding model

from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters  import RecursiveCharacterTextSplitter

print("Loading the document ...")
print("\n")
print("-"*100)


lstr_file_path="C:\Rajendra_2015\AgenticAI_Programs\Agentic_Batch2\2-Langchain Basics\2.5_LangChain_Graph\data2\."


# Get the path of the current file
current_file_path = os.path.abspath(__file__)
# If you only need the directory containing the current file:
current_file_directory = os.path.dirname(current_file_path)
data_file_path = current_file_directory + "\data2"

print(data_file_path)

print("\n")
print("-"*100)
print("\n")

print("-"*100)

#loader=DirectoryLoader("./data2",glob="./*.txt",loader_cls=TextLoader)
loader=DirectoryLoader(data_file_path,glob="./*.txt",loader_cls=TextLoader)

docs=loader.load()
print(docs)


print("Document text file loading is completed ...")
print("\n")
print("-"*100)

docs[0].page_content
print("\n")
print("\n")
print("-"*100)

text_splitter=RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50
)

new_docs=text_splitter.split_documents(documents=docs)
print(new_docs)
print("-"*100)
print("\n")
print("-"*100)


print("Text Splitting is completed ...")
print("\n")
print("-"*100)

doc_string=[doc.page_content for doc in new_docs]
print(doc_string)
len(doc_string)
print("\n")
print("\n")




db=Chroma.from_documents(new_docs,embeddings)

print("Chroma setup is completed ...")
print("\n")
print("-"*100)

retriever=db.as_retriever(search_kwargs={"k": 10})
retriever.invoke("industrial growth of usa?")

print("Retriever Invoke First Time  ...")
print("\n")
print("-"*100)
print("\n")



class TopicSelectionParser(BaseModel):
    Topic:str=Field(description="selected topic")
    Reasoning:str=Field(description='Reasoning behind topic selection')

from langchain_core.output_parsers import PydanticOutputParser
parser=PydanticOutputParser(pydantic_object=TopicSelectionParser)
parser.get_format_instructions()

print("\n")
print("-"*100)

Agentstate={}
Agentstate["messages"]=[]
print(Agentstate)

print("\n")
print("\n")

print("-"*100)
Agentstate["messages"].append("hi how are you?")
print(Agentstate)

print("\n")
print("\n")
print("-"*100)

Agentstate["messages"].append("i hope everything fine")
print(Agentstate)

print(Agentstate["messages"][-1])

print("\n")
print("\n")
print("-"*100)

print(Agentstate["messages"][0])

print("\n")
print("\n")
print("-"*100)


class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]

state={"messages":["hi"]}
state="hi"

print("\n")
print("\n")
print("-"*100)

print("Before : function_1 ...")
print("\n")
print("-"*100)

def function_1(state:AgentState):
    
    question=state["messages"][-1]
    
    print("Question",question)
    
    template="""
    Your task is to classify the given user query into one of the following categories: [USA,Not Related]. 
    Only respond with the category name and nothing else.

    User query: {question}
    {format_instructions}
    """
    
    prompt= PromptTemplate(
        template=template,
        input_variable=["question"],
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )
    
    
    chain= prompt | model | parser    
    response = chain.invoke({"question":question})    
    print("Function #1 : Parsed response:", response)    
    return {"messages": [response.Topic]}


state={"messages":["what is a today weather?"]}

state={"messages":["what is a GDP of usa??"]}

print("Calling Function #1 for query and output.....")
print("\n")
print("-"*100)
print(function_1(state))


print("\n")
print("\n")
print("-"*100)


class TopicSelectionParser(BaseModel):
    Topic:str=Field(description="selected topic")
    Reasoning:str=Field(description='Reasoning behind topic selection')


def router(state:AgentState):
    print("-> ROUTER ->")
    
    last_message=state["messages"][-1]
    print("last_message:", last_message)
    
    if "usa" in last_message.lower():
        return "RAG Call"
    else:
        return "LLM Call"
    


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# RAG Function
def function_2(state:AgentState):
    print("-> RAG Call ->")
    
    question = state["messages"][0]
    
    prompt=PromptTemplate(
        template="""You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:""",
        
        input_variables=['context', 'question']
    )
    
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | model
        | StrOutputParser()
    )
    result = rag_chain.invoke(question)    
    print("Function 2 : RAG Chain Ouput .. :", result)   
    return  {"messages": [result]}


# LLM Function
def function_3(state:AgentState):
    print("-> LLM Call ->")
    question = state["messages"][0]
    
    # Normal LLM call
    complete_query = "Anwer the follow question with you knowledge of the real world. Following is the user question: " + question
    response = model.invoke(complete_query)
    print("Function : this is the LLM Output ... :", response.content)   
    return {"messages": [response.content]}


print("Before : function_1, funtion_2 , function_3 crossed ...")
print("\n")
print("-"*100)


from langgraph.graph import StateGraph,END

workflow=StateGraph(AgentState)

workflow.add_node("Supervisor",function_1)

workflow.add_node("RAG",function_2)

workflow.add_node("LLM",function_3)


print("Nodes adding completed.... ...")
print("\n")
print("-"*100)

workflow.set_entry_point("Supervisor")

print("Entry point : Supervisor.... ...")
print("\n")
print("-"*100)

workflow.add_conditional_edges(
    "Supervisor",
    router,
    {
        "RAG Call": "RAG",
        "LLM Call": "LLM",
    }
)


print("Adding Edges.... ...")
print("\n")
print("-"*100)

workflow.add_edge("RAG",END)
workflow.add_edge("LLM",END)

app=workflow.compile()

# workflow.compile()


print("compile completed.... ...")
print("\n")
print("-"*100)

state={"messages":["hi"]}

print("\n")
print("\n")
print("-"*100)

app.invoke(state)
state={"messages":["what is a gdp of usa?"]}
app.invoke(state)

print("\n")
print("\n")
print("-"*100)

state={"messages":["can you tell me the industrial growth of world's most powerful economy?"]}

print("\n")
print("\n")
print("-"*100)


state={"messages":["can you tell me the industrial growth of world's poor economy?"]}

print("\n")
print("\n")
print("-"*100)

result=app.invoke(state)

print("Print Results.....")
print(result)
print("\n")
print("-"*100)

# print(results)

print("\n")
print("\n")
print("-"*100)


print("Print Results (-1) .....")
print(result["messages"][-1])
# print(results)

print("\n")
print("\n")
print("-"*100)

print("\n")
print("-"*100)
## This workbook is fully executed and it works
print("This workbook is fully executed and it works")
print("-"*100)
print("\n")





